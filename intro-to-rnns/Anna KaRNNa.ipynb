{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74, 46, 38, 37, 67, 40, 19, 49, 31,  3,  3,  3, 18, 38, 37, 37, 30,\n",
       "       49, 21, 38,  5,  6, 26,  6, 40, 23, 49, 38, 19, 40, 49, 38, 26, 26,\n",
       "       49, 38, 26,  6,  7, 40, 43, 49, 40, 44, 40, 19, 30, 49, 29, 64, 46,\n",
       "       38, 37, 37, 30, 49, 21, 38,  5,  6, 26, 30, 49,  6, 23, 49, 29, 64,\n",
       "       46, 38, 37, 37, 30, 49,  6, 64, 49,  6, 67, 23, 49, 78, 36, 64,  3,\n",
       "       36, 38, 30, 65,  3,  3, 25, 44, 40, 19, 30, 67, 46,  6, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74, 46, 38, 37, 67, 40, 19, 49, 31,  3,  3,  3, 18, 38, 37, 37, 30,\n",
       "        49, 21, 38,  5,  6, 26,  6, 40, 23, 49, 38, 19, 40, 49, 38, 26, 26,\n",
       "        49, 38, 26,  6,  7, 40, 43, 49, 40, 44, 40, 19, 30, 49, 29, 64],\n",
       "       [49, 38,  5, 49, 64, 78, 67, 49, 69, 78,  6, 64, 69, 49, 67, 78, 49,\n",
       "        23, 67, 38, 30, 60, 59, 49, 38, 64, 23, 36, 40, 19, 40, 57, 49, 77,\n",
       "        64, 64, 38, 60, 49, 23,  5,  6, 26,  6, 64, 69, 60, 49, 75, 29],\n",
       "       [44,  6, 64, 65,  3,  3, 59, 35, 40, 23, 60, 49,  6, 67, 76, 23, 49,\n",
       "        23, 40, 67, 67, 26, 40, 57, 65, 49, 56, 46, 40, 49, 37, 19,  6, 63,\n",
       "        40, 49,  6, 23, 49,  5, 38, 69, 64,  6, 21,  6, 63, 40, 64, 67],\n",
       "       [64, 49, 57, 29, 19,  6, 64, 69, 49, 46,  6, 23, 49, 63, 78, 64, 44,\n",
       "        40, 19, 23, 38, 67,  6, 78, 64, 49, 36,  6, 67, 46, 49, 46,  6, 23,\n",
       "         3, 75, 19, 78, 67, 46, 40, 19, 49, 36, 38, 23, 49, 67, 46,  6],\n",
       "       [49,  6, 67, 49,  6, 23, 60, 49, 23,  6, 19, 27, 59, 49, 23, 38,  6,\n",
       "        57, 49, 67, 46, 40, 49, 78, 26, 57, 49,  5, 38, 64, 60, 49, 69, 40,\n",
       "        67, 67,  6, 64, 69, 49, 29, 37, 60, 49, 38, 64, 57,  3, 63, 19],\n",
       "       [49, 72, 67, 49, 36, 38, 23,  3, 78, 64, 26, 30, 49, 36, 46, 40, 64,\n",
       "        49, 67, 46, 40, 49, 23, 38,  5, 40, 49, 40, 44, 40, 64,  6, 64, 69,\n",
       "        49, 46, 40, 49, 63, 38,  5, 40, 49, 67, 78, 49, 67, 46, 40,  6],\n",
       "       [46, 40, 64, 49, 63, 78,  5, 40, 49, 21, 78, 19, 49,  5, 40, 60, 59,\n",
       "        49, 23, 46, 40, 49, 23, 38,  6, 57, 60, 49, 38, 64, 57, 49, 36, 40,\n",
       "        64, 67, 49, 75, 38, 63,  7, 49,  6, 64, 67, 78, 49, 67, 46, 40],\n",
       "       [43, 49, 75, 29, 67, 49, 64, 78, 36, 49, 23, 46, 40, 49, 36, 78, 29,\n",
       "        26, 57, 49, 19, 40, 38, 57,  6, 26, 30, 49, 46, 38, 44, 40, 49, 23,\n",
       "        38, 63, 19,  6, 21,  6, 63, 40, 57, 60, 49, 64, 78, 67, 49,  5],\n",
       "       [67, 49,  6, 23, 64, 76, 67, 65, 49, 56, 46, 40, 30, 76, 19, 40, 49,\n",
       "        37, 19, 78, 37, 19,  6, 40, 67, 78, 19, 23, 49, 78, 21, 49, 38, 49,\n",
       "        23, 78, 19, 67, 60,  3, 75, 29, 67, 49, 36, 40, 76, 19, 40, 49],\n",
       "       [49, 23, 38,  6, 57, 49, 67, 78, 49, 46, 40, 19, 23, 40, 26, 21, 60,\n",
       "        49, 38, 64, 57, 49, 75, 40, 69, 38, 64, 49, 38, 69, 38,  6, 64, 49,\n",
       "        21, 19, 78,  5, 49, 67, 46, 40, 49, 75, 40, 69,  6, 64, 64,  6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/35720 Training loss: 4.4188 2.8153 sec/batch\n",
      "Epoch 1/20  Iteration 2/35720 Training loss: 4.3801 3.0361 sec/batch\n",
      "Epoch 1/20  Iteration 3/35720 Training loss: 4.2338 3.1443 sec/batch\n",
      "Epoch 1/20  Iteration 4/35720 Training loss: 4.4812 2.9854 sec/batch\n",
      "Epoch 1/20  Iteration 5/35720 Training loss: 4.3928 2.9427 sec/batch\n",
      "Epoch 1/20  Iteration 6/35720 Training loss: 4.2966 2.8647 sec/batch\n",
      "Epoch 1/20  Iteration 7/35720 Training loss: 4.1999 2.8765 sec/batch\n",
      "Epoch 1/20  Iteration 8/35720 Training loss: 4.1106 2.8712 sec/batch\n",
      "Epoch 1/20  Iteration 9/35720 Training loss: 4.0379 2.8966 sec/batch\n",
      "Epoch 1/20  Iteration 10/35720 Training loss: 3.9786 2.8191 sec/batch\n",
      "Epoch 1/20  Iteration 11/35720 Training loss: 3.9303 2.8637 sec/batch\n",
      "Epoch 1/20  Iteration 12/35720 Training loss: 3.8885 2.8479 sec/batch\n",
      "Epoch 1/20  Iteration 13/35720 Training loss: 3.8469 2.8739 sec/batch\n",
      "Epoch 1/20  Iteration 14/35720 Training loss: 3.8084 2.8487 sec/batch\n",
      "Epoch 1/20  Iteration 15/35720 Training loss: 3.7788 2.8464 sec/batch\n",
      "Epoch 1/20  Iteration 16/35720 Training loss: 3.7523 3.0067 sec/batch\n",
      "Epoch 1/20  Iteration 17/35720 Training loss: 3.7281 2.9121 sec/batch\n",
      "Epoch 1/20  Iteration 18/35720 Training loss: 3.7049 2.8592 sec/batch\n",
      "Epoch 1/20  Iteration 19/35720 Training loss: 3.6858 2.8659 sec/batch\n",
      "Epoch 1/20  Iteration 20/35720 Training loss: 3.6619 2.8821 sec/batch\n",
      "Epoch 1/20  Iteration 21/35720 Training loss: 3.6443 2.8635 sec/batch\n",
      "Epoch 1/20  Iteration 22/35720 Training loss: 3.6295 2.8998 sec/batch\n",
      "Epoch 1/20  Iteration 23/35720 Training loss: 3.6131 2.8713 sec/batch\n",
      "Epoch 1/20  Iteration 24/35720 Training loss: 3.5970 2.9168 sec/batch\n",
      "Epoch 1/20  Iteration 25/35720 Training loss: 3.5828 2.9487 sec/batch\n",
      "Epoch 1/20  Iteration 26/35720 Training loss: 3.5691 2.9331 sec/batch\n",
      "Epoch 1/20  Iteration 27/35720 Training loss: 3.5561 3.0139 sec/batch\n",
      "Epoch 1/20  Iteration 28/35720 Training loss: 3.5447 2.9597 sec/batch\n",
      "Epoch 1/20  Iteration 29/35720 Training loss: 3.5328 3.0082 sec/batch\n",
      "Epoch 1/20  Iteration 30/35720 Training loss: 3.5216 3.2093 sec/batch\n",
      "Epoch 1/20  Iteration 31/35720 Training loss: 3.5122 3.0491 sec/batch\n",
      "Epoch 1/20  Iteration 32/35720 Training loss: 3.5018 3.1091 sec/batch\n",
      "Epoch 1/20  Iteration 33/35720 Training loss: 3.4915 2.9687 sec/batch\n",
      "Epoch 1/20  Iteration 34/35720 Training loss: 3.4831 2.9376 sec/batch\n",
      "Epoch 1/20  Iteration 35/35720 Training loss: 3.4741 2.8945 sec/batch\n",
      "Epoch 1/20  Iteration 36/35720 Training loss: 3.4665 2.9189 sec/batch\n",
      "Epoch 1/20  Iteration 37/35720 Training loss: 3.4577 3.1229 sec/batch\n",
      "Epoch 1/20  Iteration 38/35720 Training loss: 3.4509 3.1612 sec/batch\n",
      "Epoch 1/20  Iteration 39/35720 Training loss: 3.4445 3.0632 sec/batch\n",
      "Epoch 1/20  Iteration 40/35720 Training loss: 3.4386 3.0301 sec/batch\n",
      "Epoch 1/20  Iteration 41/35720 Training loss: 3.4343 3.0144 sec/batch\n",
      "Epoch 1/20  Iteration 42/35720 Training loss: 3.4276 2.9782 sec/batch\n",
      "Epoch 1/20  Iteration 43/35720 Training loss: 3.4224 3.0135 sec/batch\n",
      "Epoch 1/20  Iteration 44/35720 Training loss: 3.4160 3.0749 sec/batch\n",
      "Epoch 1/20  Iteration 45/35720 Training loss: 3.4102 3.0520 sec/batch\n",
      "Epoch 1/20  Iteration 46/35720 Training loss: 3.4053 3.0170 sec/batch\n",
      "Epoch 1/20  Iteration 47/35720 Training loss: 3.4009 2.9710 sec/batch\n",
      "Epoch 1/20  Iteration 48/35720 Training loss: 3.3962 2.9606 sec/batch\n",
      "Epoch 1/20  Iteration 49/35720 Training loss: 3.3918 2.9519 sec/batch\n",
      "Epoch 1/20  Iteration 50/35720 Training loss: 3.3874 2.9767 sec/batch\n",
      "Epoch 1/20  Iteration 51/35720 Training loss: 3.3832 2.9847 sec/batch\n",
      "Epoch 1/20  Iteration 52/35720 Training loss: 3.3780 2.9476 sec/batch\n",
      "Epoch 1/20  Iteration 53/35720 Training loss: 3.3754 2.9145 sec/batch\n",
      "Epoch 1/20  Iteration 54/35720 Training loss: 3.3721 2.9075 sec/batch\n",
      "Epoch 1/20  Iteration 55/35720 Training loss: 3.3689 2.9233 sec/batch\n",
      "Epoch 1/20  Iteration 56/35720 Training loss: 3.3650 2.9114 sec/batch\n",
      "Epoch 1/20  Iteration 57/35720 Training loss: 3.3617 2.9014 sec/batch\n",
      "Epoch 1/20  Iteration 58/35720 Training loss: 3.3594 2.9086 sec/batch\n",
      "Epoch 1/20  Iteration 59/35720 Training loss: 3.3555 2.9242 sec/batch\n",
      "Epoch 1/20  Iteration 60/35720 Training loss: 3.3522 2.9148 sec/batch\n",
      "Epoch 1/20  Iteration 61/35720 Training loss: 3.3494 2.9987 sec/batch\n",
      "Epoch 1/20  Iteration 62/35720 Training loss: 3.3464 3.2027 sec/batch\n",
      "Epoch 1/20  Iteration 63/35720 Training loss: 3.3432 2.9529 sec/batch\n",
      "Epoch 1/20  Iteration 64/35720 Training loss: 3.3398 2.9602 sec/batch\n",
      "Epoch 1/20  Iteration 65/35720 Training loss: 3.3363 3.0022 sec/batch\n",
      "Epoch 1/20  Iteration 66/35720 Training loss: 3.3326 3.0286 sec/batch\n",
      "Epoch 1/20  Iteration 67/35720 Training loss: 3.3295 3.1106 sec/batch\n",
      "Epoch 1/20  Iteration 68/35720 Training loss: 3.3272 3.0150 sec/batch\n",
      "Epoch 1/20  Iteration 69/35720 Training loss: 3.3252 3.0375 sec/batch\n",
      "Epoch 1/20  Iteration 70/35720 Training loss: 3.3230 3.1692 sec/batch\n",
      "Epoch 1/20  Iteration 71/35720 Training loss: 3.3207 3.1960 sec/batch\n",
      "Epoch 1/20  Iteration 72/35720 Training loss: 3.3184 2.9881 sec/batch\n",
      "Epoch 1/20  Iteration 73/35720 Training loss: 3.3157 2.9778 sec/batch\n",
      "Epoch 1/20  Iteration 74/35720 Training loss: 3.3130 2.9744 sec/batch\n",
      "Epoch 1/20  Iteration 75/35720 Training loss: 3.3103 3.0354 sec/batch\n",
      "Epoch 1/20  Iteration 76/35720 Training loss: 3.3080 3.0136 sec/batch\n",
      "Epoch 1/20  Iteration 77/35720 Training loss: 3.3052 2.9597 sec/batch\n",
      "Epoch 1/20  Iteration 78/35720 Training loss: 3.3037 3.1204 sec/batch\n",
      "Epoch 1/20  Iteration 79/35720 Training loss: 3.3012 3.1420 sec/batch\n",
      "Epoch 1/20  Iteration 80/35720 Training loss: 3.2997 3.0207 sec/batch\n",
      "Epoch 1/20  Iteration 81/35720 Training loss: 3.2970 2.9155 sec/batch\n",
      "Epoch 1/20  Iteration 82/35720 Training loss: 3.2946 3.0820 sec/batch\n",
      "Epoch 1/20  Iteration 83/35720 Training loss: 3.2922 3.1096 sec/batch\n",
      "Epoch 1/20  Iteration 84/35720 Training loss: 3.2902 2.9629 sec/batch\n",
      "Epoch 1/20  Iteration 85/35720 Training loss: 3.2890 3.0173 sec/batch\n",
      "Epoch 1/20  Iteration 86/35720 Training loss: 3.2872 2.9191 sec/batch\n",
      "Epoch 1/20  Iteration 87/35720 Training loss: 3.2859 2.9386 sec/batch\n",
      "Epoch 1/20  Iteration 88/35720 Training loss: 3.2839 2.9260 sec/batch\n",
      "Epoch 1/20  Iteration 89/35720 Training loss: 3.2820 2.9318 sec/batch\n",
      "Epoch 1/20  Iteration 90/35720 Training loss: 3.2801 2.9264 sec/batch\n",
      "Epoch 1/20  Iteration 91/35720 Training loss: 3.2778 2.9597 sec/batch\n",
      "Epoch 1/20  Iteration 92/35720 Training loss: 3.2756 2.9804 sec/batch\n",
      "Epoch 1/20  Iteration 93/35720 Training loss: 3.2740 2.9350 sec/batch\n",
      "Epoch 1/20  Iteration 94/35720 Training loss: 3.2719 2.9492 sec/batch\n",
      "Epoch 1/20  Iteration 95/35720 Training loss: 3.2705 3.0806 sec/batch\n",
      "Epoch 1/20  Iteration 96/35720 Training loss: 3.2686 3.0573 sec/batch\n",
      "Epoch 1/20  Iteration 97/35720 Training loss: 3.2680 2.9468 sec/batch\n",
      "Epoch 1/20  Iteration 98/35720 Training loss: 3.2671 2.9038 sec/batch\n",
      "Epoch 1/20  Iteration 99/35720 Training loss: 3.2654 3.0091 sec/batch\n",
      "Epoch 1/20  Iteration 100/35720 Training loss: 3.2648 3.3337 sec/batch\n",
      "Epoch 1/20  Iteration 101/35720 Training loss: 3.2632 3.5704 sec/batch\n",
      "Epoch 1/20  Iteration 102/35720 Training loss: 3.2622 3.6234 sec/batch\n",
      "Epoch 1/20  Iteration 103/35720 Training loss: 3.2604 3.7105 sec/batch\n",
      "Epoch 1/20  Iteration 104/35720 Training loss: 3.2591 3.5944 sec/batch\n",
      "Epoch 1/20  Iteration 105/35720 Training loss: 3.2573 3.2783 sec/batch\n",
      "Epoch 1/20  Iteration 106/35720 Training loss: 3.2557 3.2852 sec/batch\n",
      "Epoch 1/20  Iteration 107/35720 Training loss: 3.2541 3.2968 sec/batch\n",
      "Epoch 1/20  Iteration 108/35720 Training loss: 3.2524 3.2524 sec/batch\n",
      "Epoch 1/20  Iteration 109/35720 Training loss: 3.2507 3.2538 sec/batch\n",
      "Epoch 1/20  Iteration 110/35720 Training loss: 3.2489 3.2642 sec/batch\n",
      "Epoch 1/20  Iteration 111/35720 Training loss: 3.2476 3.2702 sec/batch\n",
      "Epoch 1/20  Iteration 112/35720 Training loss: 3.2461 3.2407 sec/batch\n",
      "Epoch 1/20  Iteration 113/35720 Training loss: 3.2451 3.2527 sec/batch\n",
      "Epoch 1/20  Iteration 114/35720 Training loss: 3.2445 3.2642 sec/batch\n",
      "Epoch 1/20  Iteration 115/35720 Training loss: 3.2431 3.2698 sec/batch\n",
      "Epoch 1/20  Iteration 116/35720 Training loss: 3.2416 3.2397 sec/batch\n",
      "Epoch 1/20  Iteration 117/35720 Training loss: 3.2404 3.2727 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 118/35720 Training loss: 3.2392 3.2752 sec/batch\n",
      "Epoch 1/20  Iteration 119/35720 Training loss: 3.2381 3.2502 sec/batch\n",
      "Epoch 1/20  Iteration 120/35720 Training loss: 3.2365 3.2552 sec/batch\n",
      "Epoch 1/20  Iteration 121/35720 Training loss: 3.2357 3.2863 sec/batch\n",
      "Epoch 1/20  Iteration 122/35720 Training loss: 3.2354 3.2753 sec/batch\n",
      "Epoch 1/20  Iteration 123/35720 Training loss: 3.2344 3.2452 sec/batch\n",
      "Epoch 1/20  Iteration 124/35720 Training loss: 3.2327 3.2452 sec/batch\n",
      "Epoch 1/20  Iteration 125/35720 Training loss: 3.2311 3.2632 sec/batch\n",
      "Epoch 1/20  Iteration 126/35720 Training loss: 3.2299 3.2842 sec/batch\n",
      "Epoch 1/20  Iteration 127/35720 Training loss: 3.2285 3.2477 sec/batch\n",
      "Epoch 1/20  Iteration 128/35720 Training loss: 3.2270 3.2767 sec/batch\n",
      "Epoch 1/20  Iteration 129/35720 Training loss: 3.2255 3.2783 sec/batch\n",
      "Epoch 1/20  Iteration 130/35720 Training loss: 3.2243 3.2402 sec/batch\n",
      "Epoch 1/20  Iteration 131/35720 Training loss: 3.2226 3.2752 sec/batch\n",
      "Epoch 1/20  Iteration 132/35720 Training loss: 3.2210 3.2913 sec/batch\n",
      "Epoch 1/20  Iteration 133/35720 Training loss: 3.2192 3.2612 sec/batch\n",
      "Epoch 1/20  Iteration 134/35720 Training loss: 3.2175 3.2802 sec/batch\n",
      "Epoch 1/20  Iteration 135/35720 Training loss: 3.2159 3.2888 sec/batch\n",
      "Epoch 1/20  Iteration 136/35720 Training loss: 3.2144 3.3043 sec/batch\n",
      "Epoch 1/20  Iteration 137/35720 Training loss: 3.2131 3.2613 sec/batch\n",
      "Epoch 1/20  Iteration 138/35720 Training loss: 3.2113 3.2958 sec/batch\n",
      "Epoch 1/20  Iteration 139/35720 Training loss: 3.2099 3.2652 sec/batch\n",
      "Epoch 1/20  Iteration 140/35720 Training loss: 3.2080 3.2973 sec/batch\n",
      "Epoch 1/20  Iteration 141/35720 Training loss: 3.2065 3.2778 sec/batch\n",
      "Epoch 1/20  Iteration 142/35720 Training loss: 3.2048 3.2837 sec/batch\n",
      "Epoch 1/20  Iteration 143/35720 Training loss: 3.2029 3.5606 sec/batch\n",
      "Epoch 1/20  Iteration 144/35720 Training loss: 3.2012 3.7814 sec/batch\n",
      "Epoch 1/20  Iteration 145/35720 Training loss: 3.1990 3.5554 sec/batch\n",
      "Epoch 1/20  Iteration 146/35720 Training loss: 3.1970 3.1588 sec/batch\n",
      "Epoch 1/20  Iteration 147/35720 Training loss: 3.1952 3.1272 sec/batch\n",
      "Epoch 1/20  Iteration 148/35720 Training loss: 3.1936 2.9375 sec/batch\n",
      "Epoch 1/20  Iteration 149/35720 Training loss: 3.1915 3.1927 sec/batch\n",
      "Epoch 1/20  Iteration 150/35720 Training loss: 3.1897 3.2621 sec/batch\n",
      "Epoch 1/20  Iteration 151/35720 Training loss: 3.1882 3.2154 sec/batch\n",
      "Epoch 1/20  Iteration 152/35720 Training loss: 3.1860 3.1456 sec/batch\n",
      "Epoch 1/20  Iteration 153/35720 Training loss: 3.1844 3.2461 sec/batch\n",
      "Epoch 1/20  Iteration 154/35720 Training loss: 3.1829 3.0798 sec/batch\n",
      "Epoch 1/20  Iteration 155/35720 Training loss: 3.1810 2.9278 sec/batch\n",
      "Epoch 1/20  Iteration 156/35720 Training loss: 3.1795 3.1115 sec/batch\n",
      "Epoch 1/20  Iteration 157/35720 Training loss: 3.1776 3.4002 sec/batch\n",
      "Epoch 1/20  Iteration 158/35720 Training loss: 3.1759 3.3658 sec/batch\n",
      "Epoch 1/20  Iteration 159/35720 Training loss: 3.1744 3.2692 sec/batch\n",
      "Epoch 1/20  Iteration 160/35720 Training loss: 3.1726 3.2890 sec/batch\n",
      "Epoch 1/20  Iteration 161/35720 Training loss: 3.1707 3.3393 sec/batch\n",
      "Epoch 1/20  Iteration 162/35720 Training loss: 3.1689 3.2598 sec/batch\n",
      "Epoch 1/20  Iteration 163/35720 Training loss: 3.1669 3.2872 sec/batch\n",
      "Epoch 1/20  Iteration 164/35720 Training loss: 3.1650 3.2623 sec/batch\n",
      "Epoch 1/20  Iteration 165/35720 Training loss: 3.1633 3.2522 sec/batch\n",
      "Epoch 1/20  Iteration 166/35720 Training loss: 3.1616 3.2358 sec/batch\n",
      "Epoch 1/20  Iteration 167/35720 Training loss: 3.1596 3.2783 sec/batch\n",
      "Epoch 1/20  Iteration 168/35720 Training loss: 3.1582 3.2822 sec/batch\n",
      "Epoch 1/20  Iteration 169/35720 Training loss: 3.1561 3.2562 sec/batch\n",
      "Epoch 1/20  Iteration 170/35720 Training loss: 3.1542 3.2488 sec/batch\n",
      "Epoch 1/20  Iteration 171/35720 Training loss: 3.1528 3.2855 sec/batch\n",
      "Epoch 1/20  Iteration 172/35720 Training loss: 3.1514 3.2529 sec/batch\n",
      "Epoch 1/20  Iteration 173/35720 Training loss: 3.1496 3.2611 sec/batch\n",
      "Epoch 1/20  Iteration 174/35720 Training loss: 3.1479 3.2537 sec/batch\n",
      "Epoch 1/20  Iteration 175/35720 Training loss: 3.1458 3.2703 sec/batch\n",
      "Epoch 1/20  Iteration 176/35720 Training loss: 3.1439 3.2489 sec/batch\n",
      "Epoch 1/20  Iteration 177/35720 Training loss: 3.1422 3.2368 sec/batch\n",
      "Epoch 1/20  Iteration 178/35720 Training loss: 3.1404 3.2406 sec/batch\n",
      "Epoch 1/20  Iteration 179/35720 Training loss: 3.1387 3.2523 sec/batch\n",
      "Epoch 1/20  Iteration 180/35720 Training loss: 3.1367 3.2618 sec/batch\n",
      "Epoch 1/20  Iteration 181/35720 Training loss: 3.1347 3.2683 sec/batch\n",
      "Epoch 1/20  Iteration 182/35720 Training loss: 3.1330 3.2743 sec/batch\n",
      "Epoch 1/20  Iteration 183/35720 Training loss: 3.1308 3.2308 sec/batch\n",
      "Epoch 1/20  Iteration 184/35720 Training loss: 3.1287 3.2366 sec/batch\n",
      "Epoch 1/20  Iteration 185/35720 Training loss: 3.1267 3.2439 sec/batch\n",
      "Epoch 1/20  Iteration 186/35720 Training loss: 3.1244 3.2633 sec/batch\n",
      "Epoch 1/20  Iteration 187/35720 Training loss: 3.1220 3.2337 sec/batch\n",
      "Epoch 1/20  Iteration 188/35720 Training loss: 3.1199 3.2653 sec/batch\n",
      "Epoch 1/20  Iteration 189/35720 Training loss: 3.1178 3.2643 sec/batch\n",
      "Epoch 1/20  Iteration 190/35720 Training loss: 3.1154 3.2253 sec/batch\n",
      "Epoch 1/20  Iteration 191/35720 Training loss: 3.1131 3.2563 sec/batch\n",
      "Epoch 1/20  Iteration 192/35720 Training loss: 3.1113 3.2613 sec/batch\n",
      "Epoch 1/20  Iteration 193/35720 Training loss: 3.1092 3.4378 sec/batch\n",
      "Epoch 1/20  Iteration 194/35720 Training loss: 3.1071 2.9908 sec/batch\n",
      "Epoch 1/20  Iteration 195/35720 Training loss: 3.1047 2.9610 sec/batch\n",
      "Epoch 1/20  Iteration 196/35720 Training loss: 3.1024 3.3458 sec/batch\n",
      "Epoch 1/20  Iteration 197/35720 Training loss: 3.1004 3.5143 sec/batch\n",
      "Epoch 1/20  Iteration 198/35720 Training loss: 3.0980 3.4112 sec/batch\n",
      "Epoch 1/20  Iteration 199/35720 Training loss: 3.0961 3.1398 sec/batch\n",
      "Epoch 1/20  Iteration 200/35720 Training loss: 3.0937 2.9356 sec/batch\n",
      "Validation loss: 2.64981 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 201/35720 Training loss: 3.0913 2.6821 sec/batch\n",
      "Epoch 1/20  Iteration 202/35720 Training loss: 3.0891 3.2530 sec/batch\n",
      "Epoch 1/20  Iteration 203/35720 Training loss: 3.0867 3.2841 sec/batch\n",
      "Epoch 1/20  Iteration 204/35720 Training loss: 3.0846 3.3948 sec/batch\n",
      "Epoch 1/20  Iteration 205/35720 Training loss: 3.0823 3.3294 sec/batch\n",
      "Epoch 1/20  Iteration 206/35720 Training loss: 3.0803 3.2691 sec/batch\n",
      "Epoch 1/20  Iteration 207/35720 Training loss: 3.0779 3.2308 sec/batch\n",
      "Epoch 1/20  Iteration 208/35720 Training loss: 3.0753 3.2222 sec/batch\n",
      "Epoch 1/20  Iteration 209/35720 Training loss: 3.0728 3.2909 sec/batch\n",
      "Epoch 1/20  Iteration 210/35720 Training loss: 3.0704 3.2931 sec/batch\n",
      "Epoch 1/20  Iteration 211/35720 Training loss: 3.0682 3.6325 sec/batch\n",
      "Epoch 1/20  Iteration 212/35720 Training loss: 3.0659 3.4832 sec/batch\n",
      "Epoch 1/20  Iteration 213/35720 Training loss: 3.0636 3.3886 sec/batch\n",
      "Epoch 1/20  Iteration 214/35720 Training loss: 3.0609 3.3137 sec/batch\n",
      "Epoch 1/20  Iteration 215/35720 Training loss: 3.0584 3.3739 sec/batch\n",
      "Epoch 1/20  Iteration 216/35720 Training loss: 3.0559 3.3298 sec/batch\n",
      "Epoch 1/20  Iteration 217/35720 Training loss: 3.0536 3.4383 sec/batch\n",
      "Epoch 1/20  Iteration 218/35720 Training loss: 3.0510 3.3168 sec/batch\n",
      "Epoch 1/20  Iteration 219/35720 Training loss: 3.0487 3.3598 sec/batch\n",
      "Epoch 1/20  Iteration 220/35720 Training loss: 3.0464 3.3482 sec/batch\n",
      "Epoch 1/20  Iteration 221/35720 Training loss: 3.0441 3.4438 sec/batch\n",
      "Epoch 1/20  Iteration 222/35720 Training loss: 3.0418 3.4533 sec/batch\n",
      "Epoch 1/20  Iteration 223/35720 Training loss: 3.0395 3.0899 sec/batch\n",
      "Epoch 1/20  Iteration 224/35720 Training loss: 3.0371 3.0154 sec/batch\n",
      "Epoch 1/20  Iteration 225/35720 Training loss: 3.0348 3.0188 sec/batch\n",
      "Epoch 1/20  Iteration 226/35720 Training loss: 3.0325 3.1762 sec/batch\n",
      "Epoch 1/20  Iteration 227/35720 Training loss: 3.0302 3.2618 sec/batch\n",
      "Epoch 1/20  Iteration 228/35720 Training loss: 3.0281 3.0475 sec/batch\n",
      "Epoch 1/20  Iteration 229/35720 Training loss: 3.0261 3.3185 sec/batch\n",
      "Epoch 1/20  Iteration 230/35720 Training loss: 3.0239 3.0350 sec/batch\n",
      "Epoch 1/20  Iteration 231/35720 Training loss: 3.0213 3.0638 sec/batch\n",
      "Epoch 1/20  Iteration 232/35720 Training loss: 3.0190 3.0248 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 233/35720 Training loss: 3.0170 3.0246 sec/batch\n",
      "Epoch 1/20  Iteration 234/35720 Training loss: 3.0144 3.1153 sec/batch\n",
      "Epoch 1/20  Iteration 235/35720 Training loss: 3.0118 2.9900 sec/batch\n",
      "Epoch 1/20  Iteration 236/35720 Training loss: 3.0096 2.9854 sec/batch\n",
      "Epoch 1/20  Iteration 237/35720 Training loss: 3.0075 3.0254 sec/batch\n",
      "Epoch 1/20  Iteration 238/35720 Training loss: 3.0052 3.0916 sec/batch\n",
      "Epoch 1/20  Iteration 239/35720 Training loss: 3.0030 3.3889 sec/batch\n",
      "Epoch 1/20  Iteration 240/35720 Training loss: 3.0007 3.4652 sec/batch\n",
      "Epoch 1/20  Iteration 241/35720 Training loss: 2.9985 3.3178 sec/batch\n",
      "Epoch 1/20  Iteration 242/35720 Training loss: 2.9965 3.3605 sec/batch\n",
      "Epoch 1/20  Iteration 243/35720 Training loss: 2.9944 3.3397 sec/batch\n",
      "Epoch 1/20  Iteration 244/35720 Training loss: 2.9925 3.3421 sec/batch\n",
      "Epoch 1/20  Iteration 245/35720 Training loss: 2.9906 3.3150 sec/batch\n",
      "Epoch 1/20  Iteration 246/35720 Training loss: 2.9884 3.2076 sec/batch\n",
      "Epoch 1/20  Iteration 247/35720 Training loss: 2.9866 3.4273 sec/batch\n",
      "Epoch 1/20  Iteration 248/35720 Training loss: 2.9843 3.1385 sec/batch\n",
      "Epoch 1/20  Iteration 249/35720 Training loss: 2.9821 3.1625 sec/batch\n",
      "Epoch 1/20  Iteration 250/35720 Training loss: 2.9802 3.0364 sec/batch\n",
      "Epoch 1/20  Iteration 251/35720 Training loss: 2.9780 3.2268 sec/batch\n",
      "Epoch 1/20  Iteration 252/35720 Training loss: 2.9758 3.2275 sec/batch\n",
      "Epoch 1/20  Iteration 253/35720 Training loss: 2.9735 3.1836 sec/batch\n",
      "Epoch 1/20  Iteration 254/35720 Training loss: 2.9715 3.1923 sec/batch\n",
      "Epoch 1/20  Iteration 255/35720 Training loss: 2.9697 3.1549 sec/batch\n",
      "Epoch 1/20  Iteration 256/35720 Training loss: 2.9680 3.1947 sec/batch\n",
      "Epoch 1/20  Iteration 257/35720 Training loss: 2.9659 3.1974 sec/batch\n",
      "Epoch 1/20  Iteration 258/35720 Training loss: 2.9641 3.2356 sec/batch\n",
      "Epoch 1/20  Iteration 259/35720 Training loss: 2.9621 3.2454 sec/batch\n",
      "Epoch 1/20  Iteration 260/35720 Training loss: 2.9602 3.1936 sec/batch\n",
      "Epoch 1/20  Iteration 261/35720 Training loss: 2.9581 3.1848 sec/batch\n",
      "Epoch 1/20  Iteration 262/35720 Training loss: 2.9562 3.1947 sec/batch\n",
      "Epoch 1/20  Iteration 263/35720 Training loss: 2.9540 3.2059 sec/batch\n",
      "Epoch 1/20  Iteration 264/35720 Training loss: 2.9520 3.0979 sec/batch\n",
      "Epoch 1/20  Iteration 265/35720 Training loss: 2.9500 3.2442 sec/batch\n",
      "Epoch 1/20  Iteration 266/35720 Training loss: 2.9481 3.1777 sec/batch\n",
      "Epoch 1/20  Iteration 267/35720 Training loss: 2.9459 3.0615 sec/batch\n",
      "Epoch 1/20  Iteration 268/35720 Training loss: 2.9439 3.0563 sec/batch\n",
      "Epoch 1/20  Iteration 269/35720 Training loss: 2.9414 3.0870 sec/batch\n",
      "Epoch 1/20  Iteration 270/35720 Training loss: 2.9391 3.0918 sec/batch\n",
      "Epoch 1/20  Iteration 271/35720 Training loss: 2.9372 3.0487 sec/batch\n",
      "Epoch 1/20  Iteration 272/35720 Training loss: 2.9351 3.0744 sec/batch\n",
      "Epoch 1/20  Iteration 273/35720 Training loss: 2.9330 3.1311 sec/batch\n",
      "Epoch 1/20  Iteration 274/35720 Training loss: 2.9313 3.0492 sec/batch\n",
      "Epoch 1/20  Iteration 275/35720 Training loss: 2.9291 3.1080 sec/batch\n",
      "Epoch 1/20  Iteration 276/35720 Training loss: 2.9279 3.1999 sec/batch\n",
      "Epoch 1/20  Iteration 277/35720 Training loss: 2.9260 3.1143 sec/batch\n",
      "Epoch 1/20  Iteration 278/35720 Training loss: 2.9242 3.0784 sec/batch\n",
      "Epoch 1/20  Iteration 279/35720 Training loss: 2.9222 3.0557 sec/batch\n",
      "Epoch 1/20  Iteration 280/35720 Training loss: 2.9206 3.2427 sec/batch\n",
      "Epoch 1/20  Iteration 281/35720 Training loss: 2.9190 3.2042 sec/batch\n",
      "Epoch 1/20  Iteration 282/35720 Training loss: 2.9172 3.0797 sec/batch\n",
      "Epoch 1/20  Iteration 283/35720 Training loss: 2.9154 3.0616 sec/batch\n",
      "Epoch 1/20  Iteration 284/35720 Training loss: 2.9137 3.1173 sec/batch\n",
      "Epoch 1/20  Iteration 285/35720 Training loss: 2.9125 3.1585 sec/batch\n",
      "Epoch 1/20  Iteration 286/35720 Training loss: 2.9110 3.1863 sec/batch\n",
      "Epoch 1/20  Iteration 287/35720 Training loss: 2.9091 3.0698 sec/batch\n",
      "Epoch 1/20  Iteration 288/35720 Training loss: 2.9071 3.0677 sec/batch\n",
      "Epoch 1/20  Iteration 289/35720 Training loss: 2.9055 3.0639 sec/batch\n",
      "Epoch 1/20  Iteration 290/35720 Training loss: 2.9041 3.0541 sec/batch\n",
      "Epoch 1/20  Iteration 291/35720 Training loss: 2.9026 3.0501 sec/batch\n",
      "Epoch 1/20  Iteration 292/35720 Training loss: 2.9011 3.0039 sec/batch\n",
      "Epoch 1/20  Iteration 293/35720 Training loss: 2.8993 3.0117 sec/batch\n",
      "Epoch 1/20  Iteration 294/35720 Training loss: 2.8973 3.1995 sec/batch\n",
      "Epoch 1/20  Iteration 295/35720 Training loss: 2.8962 3.1718 sec/batch\n",
      "Epoch 1/20  Iteration 296/35720 Training loss: 2.8945 3.0931 sec/batch\n",
      "Epoch 1/20  Iteration 297/35720 Training loss: 2.8930 3.0391 sec/batch\n",
      "Epoch 1/20  Iteration 298/35720 Training loss: 2.8917 3.1423 sec/batch\n",
      "Epoch 1/20  Iteration 299/35720 Training loss: 2.8901 3.1779 sec/batch\n",
      "Epoch 1/20  Iteration 300/35720 Training loss: 2.8882 3.1114 sec/batch\n",
      "Epoch 1/20  Iteration 301/35720 Training loss: 2.8867 3.1953 sec/batch\n",
      "Epoch 1/20  Iteration 302/35720 Training loss: 2.8850 3.0079 sec/batch\n",
      "Epoch 1/20  Iteration 303/35720 Training loss: 2.8831 3.2943 sec/batch\n",
      "Epoch 1/20  Iteration 304/35720 Training loss: 2.8816 3.2173 sec/batch\n",
      "Epoch 1/20  Iteration 305/35720 Training loss: 2.8802 3.2309 sec/batch\n",
      "Epoch 1/20  Iteration 306/35720 Training loss: 2.8787 3.2448 sec/batch\n",
      "Epoch 1/20  Iteration 307/35720 Training loss: 2.8772 3.2953 sec/batch\n",
      "Epoch 1/20  Iteration 308/35720 Training loss: 2.8754 3.3576 sec/batch\n",
      "Epoch 1/20  Iteration 309/35720 Training loss: 2.8737 3.2521 sec/batch\n",
      "Epoch 1/20  Iteration 310/35720 Training loss: 2.8718 3.3276 sec/batch\n",
      "Epoch 1/20  Iteration 311/35720 Training loss: 2.8700 3.3313 sec/batch\n",
      "Epoch 1/20  Iteration 312/35720 Training loss: 2.8684 3.3621 sec/batch\n",
      "Epoch 1/20  Iteration 313/35720 Training loss: 2.8667 3.2221 sec/batch\n",
      "Epoch 1/20  Iteration 314/35720 Training loss: 2.8650 3.5873 sec/batch\n",
      "Epoch 1/20  Iteration 315/35720 Training loss: 2.8637 3.4203 sec/batch\n",
      "Epoch 1/20  Iteration 316/35720 Training loss: 2.8621 3.3070 sec/batch\n",
      "Epoch 1/20  Iteration 317/35720 Training loss: 2.8606 3.2175 sec/batch\n",
      "Epoch 1/20  Iteration 318/35720 Training loss: 2.8594 3.0562 sec/batch\n",
      "Epoch 1/20  Iteration 319/35720 Training loss: 2.8577 3.0545 sec/batch\n",
      "Epoch 1/20  Iteration 320/35720 Training loss: 2.8559 3.0675 sec/batch\n",
      "Epoch 1/20  Iteration 321/35720 Training loss: 2.8545 3.0952 sec/batch\n",
      "Epoch 1/20  Iteration 322/35720 Training loss: 2.8528 3.0243 sec/batch\n",
      "Epoch 1/20  Iteration 323/35720 Training loss: 2.8511 3.0808 sec/batch\n",
      "Epoch 1/20  Iteration 324/35720 Training loss: 2.8496 3.0667 sec/batch\n",
      "Epoch 1/20  Iteration 325/35720 Training loss: 2.8480 3.0197 sec/batch\n",
      "Epoch 1/20  Iteration 326/35720 Training loss: 2.8463 2.9888 sec/batch\n",
      "Epoch 1/20  Iteration 327/35720 Training loss: 2.8447 3.0561 sec/batch\n",
      "Epoch 1/20  Iteration 328/35720 Training loss: 2.8431 3.0324 sec/batch\n",
      "Epoch 1/20  Iteration 329/35720 Training loss: 2.8417 3.1616 sec/batch\n",
      "Epoch 1/20  Iteration 330/35720 Training loss: 2.8402 3.1071 sec/batch\n",
      "Epoch 1/20  Iteration 331/35720 Training loss: 2.8388 3.0632 sec/batch\n",
      "Epoch 1/20  Iteration 332/35720 Training loss: 2.8375 3.1726 sec/batch\n",
      "Epoch 1/20  Iteration 333/35720 Training loss: 2.8364 3.0016 sec/batch\n",
      "Epoch 1/20  Iteration 334/35720 Training loss: 2.8350 3.0206 sec/batch\n",
      "Epoch 1/20  Iteration 335/35720 Training loss: 2.8336 3.0547 sec/batch\n",
      "Epoch 1/20  Iteration 336/35720 Training loss: 2.8323 3.1071 sec/batch\n",
      "Epoch 1/20  Iteration 337/35720 Training loss: 2.8312 3.1270 sec/batch\n",
      "Epoch 1/20  Iteration 338/35720 Training loss: 2.8295 3.0836 sec/batch\n",
      "Epoch 1/20  Iteration 339/35720 Training loss: 2.8283 3.1605 sec/batch\n",
      "Epoch 1/20  Iteration 340/35720 Training loss: 2.8269 3.0867 sec/batch\n",
      "Epoch 1/20  Iteration 341/35720 Training loss: 2.8254 3.1325 sec/batch\n",
      "Epoch 1/20  Iteration 342/35720 Training loss: 2.8237 3.0704 sec/batch\n",
      "Epoch 1/20  Iteration 343/35720 Training loss: 2.8221 3.1042 sec/batch\n",
      "Epoch 1/20  Iteration 344/35720 Training loss: 2.8205 3.1582 sec/batch\n",
      "Epoch 1/20  Iteration 345/35720 Training loss: 2.8191 3.4202 sec/batch\n",
      "Epoch 1/20  Iteration 346/35720 Training loss: 2.8177 3.3722 sec/batch\n",
      "Epoch 1/20  Iteration 347/35720 Training loss: 2.8165 3.1269 sec/batch\n",
      "Epoch 1/20  Iteration 348/35720 Training loss: 2.8151 3.2193 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
